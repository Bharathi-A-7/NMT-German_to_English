{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "German to English Translation model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgyT_iGLIida"
      },
      "source": [
        "# **Neural Machine Translation - German to English**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIVxhoS4Iwjd"
      },
      "source": [
        "# 1. Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhsGNBoQIbfK"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "import re\r\n",
        "import os\r\n",
        "import io\r\n",
        "import string\r\n",
        "import unicodedata\r\n",
        "from unicodedata import normalize\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTNjDsAJK_iW"
      },
      "source": [
        "# 2. Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB6ap7ArLeBR",
        "outputId": "9e5e2386-30eb-48d9-92e5-96cedcf3967f"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/NMT/deu-eng.zip' -d '/content/drive/MyDrive/NMT'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/NMT/deu-eng.zip\n",
            "  inflating: /content/drive/MyDrive/NMT/deu.txt  \n",
            "  inflating: /content/drive/MyDrive/NMT/_about.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHT_3bCOZpVW"
      },
      "source": [
        "Below is a tokenizer class that wraps the Keras' Tokenizer with additional functionalities. \r\n",
        "\r\n",
        "1. Reverse source langauge sequences\r\n",
        "2. Create a mapping from Index to word\r\n",
        "3. Convert tokens to string\r\n",
        "4. Convert token to word\r\n",
        "5. Text to tokens with reversal and padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGKmzroN0Ko0"
      },
      "source": [
        "class TokenizerWrap(Tokenizer):\r\n",
        "\r\n",
        "  def __init__(self, lang, padding, reverse=False, num_words=None):\r\n",
        "\r\n",
        "    # Call base class' constructor\r\n",
        "    Tokenizer.__init__(self, num_words=num_words)\r\n",
        "\r\n",
        "    self.fit_on_texts(lang)\r\n",
        "\r\n",
        "    # Inverse lookup\r\n",
        "    self.index_to_word = dict(zip(self.word_index.values(),self.word_index.keys()))\r\n",
        "\r\n",
        "    self.tensor = self.texts_to_sequences(lang)\r\n",
        "\r\n",
        "\r\n",
        "    if reverse:\r\n",
        "      self.tensor = [list(reversed(seq)) for seq in self.tensor]\r\n",
        "      truncating ='pre'\r\n",
        "\r\n",
        "    else:\r\n",
        "      truncating = 'post'\r\n",
        "\r\n",
        "    \r\n",
        "    self.num_tokens = [len(seq) for seq in self.tensor]\r\n",
        "\r\n",
        "    # Find the maximum length of a sequence\r\n",
        "    self.max_len_tokens = np.mean(self.num_tokens) + 2* np.std(self.num_tokens)\r\n",
        "\r\n",
        "    self.max_len_tokens = int(self.max_len_tokens)\r\n",
        "    # Pad the tokens to the maximum length\r\n",
        "    self.padded_tensor =  pad_sequences(self.tensor, maxlen=self.max_len_tokens, padding=padding, truncating=truncating)\r\n",
        "                                        \r\n",
        "  \r\n",
        "  def token_to_word(self, token):\r\n",
        "    ''' Converts an integer token to its corresponding word, if it exists    '''\r\n",
        "\r\n",
        "    if token==0:\r\n",
        "      word = \" \"\r\n",
        "    else:\r\n",
        "      word = self.index_to_word[token]\r\n",
        "\r\n",
        "    return word\r\n",
        "\r\n",
        "  def tokens_to_string(self, tokens, reverse=False):\r\n",
        "    ''' Converts a list of integer tokens to their corresponding text sequence  '''\r\n",
        "\r\n",
        "    words = [self.index_to_word[token] for token in tokens if token!=0]\r\n",
        "\r\n",
        "    if reverse:\r\n",
        "      words.reverse()\r\n",
        "\r\n",
        "    text = \" \".join(words)\r\n",
        "\r\n",
        "    return text\r\n",
        "\r\n",
        " \r\n",
        "  def text_to_tokens(self, text, reverse=False, padding=False):\r\n",
        "\r\n",
        "   ''' Converts text to tokens with reversal and padding '''\r\n",
        "\r\n",
        "   tokens = self.texts_to_sequences([text])\r\n",
        "   tokens = np.array(tokens)\r\n",
        "\r\n",
        "   if reverse:\r\n",
        "     tokens = np.flip(tokens,axis=1)\r\n",
        "\r\n",
        "     truncating = 'pre'\r\n",
        "\r\n",
        "   else:\r\n",
        "     truncating = 'post'\r\n",
        "\r\n",
        "   \r\n",
        "   if padding:\r\n",
        "\r\n",
        "     tokens = pad_sequences(tokens, self.max_len_tokens, padding=True, truncating=True)\r\n",
        "   return tokens\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDrW4tKwiOOc"
      },
      "source": [
        "A class called Data which is used to read the file, load the data, preprocess the text and return the tensors for source and target language.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3p9AIxILEdd"
      },
      "source": [
        "class Data:\r\n",
        "\r\n",
        "  def __init__(self, langType):\r\n",
        "\r\n",
        "    self.langType = 'deu-eng'\r\n",
        "    self.input_tokenizer = None\r\n",
        "    self.target_tokenizer = None\r\n",
        "\r\n",
        "\r\n",
        "  def preprocess_sentence(self, sentence):\r\n",
        "\r\n",
        "    #To remove non-printable characters\r\n",
        "    re_printable = re.compile('[^%s]' % re.escape(string.printable))\r\n",
        "\r\n",
        "    #Translation table to remove punctuation\r\n",
        "    transTable = str.maketrans('','',string.punctuation)\r\n",
        "\r\n",
        "    #Unicode Normalize sentence \r\n",
        "    sentence = normalize('NFD', sentence).encode('ascii','ignore')\r\n",
        "    sentence = sentence.decode('UTF-8')\r\n",
        "\r\n",
        "    sentence = sentence.split()\r\n",
        "    # Convert to lower case\r\n",
        "    sentence = [word.lower() for word in sentence]\r\n",
        "    # Remove punctuations\r\n",
        "    sentence = [word.translate(transTable) for word in sentence]\r\n",
        "    # Remove Non-printable characters\r\n",
        "    sentence = [re_printable.sub('',word) for word in sentence]\r\n",
        "    # Remove tokens that are numbers\r\n",
        "    sentence = [word for word in sentence if word.isalpha()]\r\n",
        "\r\n",
        "    # Convert list of words to a string\r\n",
        "    processedSentence = ' '.join(sentence)\r\n",
        "\r\n",
        "    processedSentence = 'ssss ' + processedSentence + ' eeee'\r\n",
        "\r\n",
        "    return processedSentence\r\n",
        "\r\n",
        "  def tokenize(self, lang, padding, reverse, num_words):\r\n",
        "\r\n",
        "    lang_tokenizer = TokenizerWrap(lang, padding, reverse, num_words=num_words)\r\n",
        "\r\n",
        "    return lang_tokenizer.padded_tensor, lang_tokenizer\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "  def readFile(self, path, num_sentences):\r\n",
        "\r\n",
        "    ''' read and preprocess sentences in the file '''\r\n",
        "\r\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\r\n",
        "\r\n",
        "    sentence_Pairs = [[self.preprocess_sentence(s) for s in l.split('\\t')[:2]]  for l in lines[:num_sentences]]\r\n",
        "\r\n",
        "    sentencePairs = []\r\n",
        "\r\n",
        "    #Reverse each pair in sentencePairs ( We need German -> English mapping)\r\n",
        "    for pair in sentence_Pairs:\r\n",
        "        pair.reverse()\r\n",
        "        sentencePairs.append(pair)\r\n",
        "\r\n",
        "\r\n",
        "    return zip(*sentencePairs)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def loadDataset(self, file_path, num_sentences):\r\n",
        "\r\n",
        "    ''' Create and return tensors and tokenizers'''\r\n",
        "\r\n",
        "    input_sentences, target_sentences = self.readFile(file_path, num_sentences)\r\n",
        "\r\n",
        "    input_tensor, input_tokenizer = self.tokenize(input_sentences, padding='pre', reverse=True, num_words=20000)\r\n",
        "    \r\n",
        "    target_tensor, target_tokenizer = self.tokenize(target_sentences, padding='post', reverse=False, num_words=20000)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "    return input_tensor, target_tensor, input_tokenizer, target_tokenizer \r\n",
        "    \r\n",
        "  \r\n",
        "  def createDataset(self, file_path, num_sentences, buffer_size, batch_size):\r\n",
        "\r\n",
        "    filePath = file_path\r\n",
        "    input_tensor, target_tensor, self.input_tokenizer, self.target_tokenizer = self.loadDataset(filePath, num_sentences)\r\n",
        "\r\n",
        "    input_tensor_train, input_tensor_test, target_tensor_train, target_tensor_test = train_test_split(input_tensor, target_tensor, test_size = 0.2)\r\n",
        "  \r\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\r\n",
        "    train_data = train_data.shuffle(buffer_size).batch(batch_size,drop_remainder=True)\r\n",
        "\r\n",
        "    test_data = tf.data.Dataset.from_tensor_slices((input_tensor_test, target_tensor_test))\r\n",
        "    test_data = test_data.batch(batch_size, drop_remainder=True)\r\n",
        "\r\n",
        "    return train_data, test_data, self.input_tokenizer, self.target_tokenizer\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i-0O01mccXi"
      },
      "source": [
        "dataObject = Data('deu-eng')\r\n",
        "batch_size = 64\r\n",
        "buffer_size = 32000\r\n",
        "num_sentences = 30000\r\n",
        "\r\n",
        "train_data, test_data, input_tokenizer, target_tokenizer = dataObject.createDataset('/content/drive/MyDrive/NMT/deu.txt',num_sentences, buffer_size, batch_size)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EVz7qKtFn9c"
      },
      "source": [
        "Let us look at a tensor.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmYhi2Z7Fviv",
        "outputId": "7e9e1aff-f8c4-41ba-fab0-8a00f26658f9"
      },
      "source": [
        "sample_input, sample_target = next(iter(train_data))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "text_s = input_tokenizer.tokens_to_string(sample_input[3].numpy(), reverse=True) # Indexing using tensor[0].numpy()\r\n",
        "text_t = target_tokenizer.tokens_to_string(sample_target[3].numpy())\r\n",
        "print(\"Source : \")\r\n",
        "print(sample_input.shape)\r\n",
        "print(text_s)\r\n",
        "\r\n",
        "print(\"Target\")\r\n",
        "print(sample_target.shape)\r\n",
        "print(text_t)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source : \n",
            "(64, 7)\n",
            "ssss lass es tom machen eeee\n",
            "Target\n",
            "(64, 6)\n",
            "ssss let tom do it eeee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2iEaHIgTpka"
      },
      "source": [
        "Setting some important parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDKs3d_tTxjv"
      },
      "source": [
        "input_vocab_size = len(input_tokenizer.word_index)+1\r\n",
        "target_vocab_size = len(target_tokenizer.word_index)+1\r\n",
        "\r\n",
        "input_max_length = sample_input.shape[1]\r\n",
        "target_max_length = sample_target.shape[1]\r\n",
        "\r\n",
        "embedding_size = 256\r\n",
        "num_hidden_units = 1024\r\n",
        "steps_per_epoch = num_sentences/batch_size\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UP33JdUWAHy",
        "outputId": "3a4c6aea-764f-4dfa-b615-b5f85def8287"
      },
      "source": [
        "print(\"Size of German vocabulary : \", input_vocab_size)\r\n",
        "print(\"Size of English vocabulary : \", target_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of German vocabulary :  7285\n",
            "Size of English vocabulary :  4620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdpLFW8xV1oe"
      },
      "source": [
        "# 3. Building the Encoder and Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Wy5NP9XK64"
      },
      "source": [
        "Below is the implementation of Encoder stack :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRrz4RlGXKN9"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "\r\n",
        "  def __init__(self, vocab_size, embedding_size, state_size, batch_size):\r\n",
        "\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "\r\n",
        "    self.state_size = state_size\r\n",
        "    self.embedding_size = embedding_size\r\n",
        "    self.batch_size = batch_size\r\n",
        "\r\n",
        "    # An Embedding layer followed by three LSTM layers in the Encoder\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\r\n",
        "    self.lstm1 = tf.keras.layers.LSTM(state_size, return_sequences=True)\r\n",
        "    self.lstm2 = tf.keras.layers.LSTM(state_size, return_sequences=True)\r\n",
        "    self.lstm3 = tf.keras.layers.LSTM(state_size, return_sequences=True, return_state=True)\r\n",
        "\r\n",
        "  \r\n",
        "  #Function to make a forward pass through the encoder, given a batch of input tokens\r\n",
        "  def call(self, input):\r\n",
        "\r\n",
        "    embedding = self.embedding(input)\r\n",
        "\r\n",
        "    out = self.lstm1(embedding)\r\n",
        "    out = self.lstm2(out)\r\n",
        "    out, h, c = self.lstm3(out)\r\n",
        "\r\n",
        "    return out, h, c\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59KrCmDO-b8e",
        "outputId": "9bbf103a-7e41-40ef-aa69-fc2252695cf6"
      },
      "source": [
        "encoder = Encoder(input_vocab_size, embedding_size, num_hidden_units, batch_size)\r\n",
        "\r\n",
        "sample_out, sample_h, sample_c = encoder(sample_input)\r\n",
        "\r\n",
        "print(sample_out.shape)\r\n",
        "print(sample_h.shape)\r\n",
        "print(sample_c.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 7, 1024)\n",
            "(64, 1024)\n",
            "(64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmmE8r9uBYPR"
      },
      "source": [
        "Below is the implementation of Decoder stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zb0GC0g7t9L"
      },
      "source": [
        "**Bahadanau Attention Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhoQGW1YBe_5"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\r\n",
        "\r\n",
        "  def __init__(self, num_units):\r\n",
        "\r\n",
        "    super(BahdanauAttention, self).__init__()\r\n",
        "\r\n",
        "    self.W1 = tf.keras.layers.Dense(num_units)\r\n",
        "    self.W2 = tf.keras.layers.Dense(num_units)\r\n",
        "    self.V = tf.keras.layers.Dense(1)\r\n",
        "\r\n",
        "  def call(self, encoder_output, previous_hidden):\r\n",
        "\r\n",
        "    previous_hidden = tf.expand_dims(previous_hidden, axis=1)\r\n",
        "\r\n",
        "    alignment_score = self.V(tf.nn.tanh(self.W1(encoder_output) + self.W2(previous_hidden)))\r\n",
        "\r\n",
        "    attention_weights = tf.nn.softmax(alignment_score, axis=1)\r\n",
        "\r\n",
        "    context_vector = attention_weights*encoder_output\r\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\r\n",
        "\r\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLnFgoVn9Sjb",
        "outputId": "d1446b08-8dcf-4468-ed36-e62565d2a72f"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\r\n",
        "\r\n",
        "context_vector, attention_weights = attention_layer(sample_out, sample_h)\r\n",
        "\r\n",
        "\r\n",
        "print('Context Vector shape : ', context_vector.shape)\r\n",
        "print('Attention weights shape : ', attention_weights.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context Vector shape :  (64, 1024)\n",
            "Attention weights shape :  (64, 7, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aAn_Y04TTya"
      },
      "source": [
        "**Decoder with Attention mechanism**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijn00LX5TYpi"
      },
      "source": [
        "class Decoder(tf.keras.Model):\r\n",
        "\r\n",
        "  def __init__(self, vocab_size, embedding_size, state_size, batch_size):\r\n",
        "\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.num_units = state_size\r\n",
        "\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\r\n",
        "\r\n",
        "    self.lstm = tf.keras.layers.LSTM(self.num_units, return_sequences=True, return_state=True)\r\n",
        "\r\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    self.attention = BahdanauAttention(self.num_units)\r\n",
        "\r\n",
        "\r\n",
        "  def call(self, input, previous_hidden, encoder_output):\r\n",
        "    \r\n",
        "\r\n",
        "    context_vector, attention_weights = self.attention(encoder_output, previous_hidden)\r\n",
        "   \r\n",
        "    input = self.embedding(input)\r\n",
        "    \r\n",
        "    input = tf.concat([tf.expand_dims(context_vector, 1), input], axis=-1)\r\n",
        "    \r\n",
        "    output, h, c = self.lstm(input)\r\n",
        "   \r\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\r\n",
        "   \r\n",
        "    output = self.dense(output)\r\n",
        "    \r\n",
        "    return output, h, attention_weights\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--2idiyM_JC7",
        "outputId": "e5d0bb9a-be47-4a16-f93c-3d32f92c05f1"
      },
      "source": [
        "decoder = Decoder(target_vocab_size, embedding_size, num_hidden_units, batch_size)\r\n",
        "\r\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)), sample_h, sample_out)\r\n",
        "\r\n",
        "print ('Decoder output' , sample_decoder_output.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output (64, 4620)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfC6aBnsdQ2e"
      },
      "source": [
        "# 4. Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKn1d1VcdVLK"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "\r\n",
        "\r\n",
        "def loss_function(truth, predicted):\r\n",
        "\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(truth, 0))\r\n",
        "\r\n",
        "  loss = loss_object(truth, predicted)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\r\n",
        "\r\n",
        "  loss = loss*mask\r\n",
        "\r\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVre3Kk5--GH"
      },
      "source": [
        "Checkpoints :\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc6ZG_ny-9X6"
      },
      "source": [
        "checkpoint_dir = './training_checkpoint'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"chkpt\")\r\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd-9VeCVDMcj"
      },
      "source": [
        "# 5. Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdtNNykEl02Z"
      },
      "source": [
        "**Training using Teacher Forcing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVh-vwLZLX1Y"
      },
      "source": [
        "Training step over a single batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoF2vwFDPU-"
      },
      "source": [
        "def train(input_tensor, target_tensor):\r\n",
        "\r\n",
        "  optimizer = tf.keras.optimizers.Adam()\r\n",
        "  loss = 0.0\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "\r\n",
        "    batch_size = input_tensor.shape[0]\r\n",
        "\r\n",
        "    encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor)\r\n",
        "\r\n",
        "    previous_hidden = encoder_hidden\r\n",
        "\r\n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['ssss']]*batch_size,1)\r\n",
        "\r\n",
        "    for t in range(1, target_tensor.shape[1]):\r\n",
        "\r\n",
        "      predictions, previous_hidden, _ = decoder(decoder_input, previous_hidden, encoder_output)\r\n",
        "\r\n",
        "      loss = loss + loss_function(target_tensor[:,t], predictions)\r\n",
        "\r\n",
        "      # Teacher forcing\r\n",
        "      decoder_input = tf.expand_dims(target_tensor[:,t],1)\r\n",
        "\r\n",
        "  # Averaging over the length of the target sequences\r\n",
        "  batch_loss = (loss / int(target_tensor.shape[1]))\r\n",
        "\r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "\r\n",
        "  gradients = tape.gradient(loss, variables)\r\n",
        "\r\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "\r\n",
        "  return batch_loss\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US1tQHeSLazl"
      },
      "source": [
        "Training over multiple batches and epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODkb_npKLM-b",
        "outputId": "3b5fb86a-0aab-4ec7-bdd5-34fcf28c3e8d"
      },
      "source": [
        "epochs = 10\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "\r\n",
        "  total_loss = 0\r\n",
        "\r\n",
        "  for (batch, (input_tensor,target_tensor)) in enumerate(train_data.take(int(steps_per_epoch))):\r\n",
        "\r\n",
        "    batch_loss = train(input_tensor, target_tensor)\r\n",
        "\r\n",
        "    total_loss = total_loss + batch_loss\r\n",
        "\r\n",
        "    if batch % 100 == 0:\r\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\r\n",
        "  # Save the model every 2 epochs\r\n",
        "  if (epoch + 1) % 2 == 0:\r\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\r\n",
        "\r\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 6.1529\n",
            "Epoch 1 Batch 100 Loss 3.9618\n",
            "Epoch 1 Batch 200 Loss 3.4144\n",
            "Epoch 1 Batch 300 Loss 3.2180\n",
            "Epoch 1 Loss 2.9579\n",
            "Epoch 2 Batch 0 Loss 3.4042\n",
            "Epoch 2 Batch 100 Loss 3.2181\n",
            "Epoch 2 Batch 200 Loss 3.0015\n",
            "Epoch 2 Batch 300 Loss 3.0240\n",
            "Epoch 2 Loss 2.4384\n",
            "Epoch 3 Batch 0 Loss 3.0415\n",
            "Epoch 3 Batch 100 Loss 2.6151\n",
            "Epoch 3 Batch 200 Loss 2.6528\n",
            "Epoch 3 Batch 300 Loss 2.6589\n",
            "Epoch 3 Loss 2.1859\n",
            "Epoch 4 Batch 0 Loss 2.6376\n",
            "Epoch 4 Batch 100 Loss 2.3059\n",
            "Epoch 4 Batch 200 Loss 2.5141\n",
            "Epoch 4 Batch 300 Loss 2.5270\n",
            "Epoch 4 Loss 1.9780\n",
            "Epoch 5 Batch 0 Loss 2.1168\n",
            "Epoch 5 Batch 100 Loss 2.1173\n",
            "Epoch 5 Batch 200 Loss 2.3309\n",
            "Epoch 5 Batch 300 Loss 2.3316\n",
            "Epoch 5 Loss 1.8241\n",
            "Epoch 6 Batch 0 Loss 2.0689\n",
            "Epoch 6 Batch 100 Loss 1.9538\n",
            "Epoch 6 Batch 200 Loss 2.0011\n",
            "Epoch 6 Batch 300 Loss 2.2671\n",
            "Epoch 6 Loss 1.7095\n",
            "Epoch 7 Batch 0 Loss 1.7719\n",
            "Epoch 7 Batch 100 Loss 1.8833\n",
            "Epoch 7 Batch 200 Loss 2.1868\n",
            "Epoch 7 Batch 300 Loss 2.0113\n",
            "Epoch 7 Loss 1.6204\n",
            "Epoch 8 Batch 0 Loss 1.8262\n",
            "Epoch 8 Batch 100 Loss 1.7720\n",
            "Epoch 8 Batch 200 Loss 2.0881\n",
            "Epoch 8 Batch 300 Loss 1.8952\n",
            "Epoch 8 Loss 1.5565\n",
            "Epoch 9 Batch 0 Loss 1.7707\n",
            "Epoch 9 Batch 100 Loss 1.8945\n",
            "Epoch 9 Batch 200 Loss 1.7639\n",
            "Epoch 9 Batch 300 Loss 1.8439\n",
            "Epoch 9 Loss 1.4977\n",
            "Epoch 10 Batch 0 Loss 1.7183\n",
            "Epoch 10 Batch 100 Loss 1.6978\n",
            "Epoch 10 Batch 200 Loss 1.8179\n",
            "Epoch 10 Batch 300 Loss 1.8075\n",
            "Epoch 10 Loss 1.4447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeqcEwqrFDhv"
      },
      "source": [
        "# 6. Testing and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bdTqdVoPs7M"
      },
      "source": [
        "Testing on batches of test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4q2oBP8FLhj"
      },
      "source": [
        "def test(input_tensor, target_tensor):\r\n",
        "\r\n",
        "  val_loss = 0.0\r\n",
        "\r\n",
        "  batch_size = input_tensor.shape[0]\r\n",
        "\r\n",
        "  encoder_output, encoder_hidden, _ = encoder(input_tensor)\r\n",
        "\r\n",
        "  decoder_hidden = encoder_hidden\r\n",
        "\r\n",
        "  decoder_input = tf.expand_dims([target_tokenizer.word_index['ssss']]*batch_size, 1)\r\n",
        "\r\n",
        "  for t in range(1, target_tensor.shape[1]):\r\n",
        "    \r\n",
        "    predictions, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_output)\r\n",
        "   \r\n",
        "    val_loss += loss_function(target_tensor[:,t], predictions)\r\n",
        "    \r\n",
        "    decoder_input = tf.expand_dims(np.argmax(predictions, axis=1),1)\r\n",
        "\r\n",
        "  batch_loss = (val_loss/int(target_tensor.shape[1]))\r\n",
        "  return batch_loss\r\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmaeUMKVPqdl"
      },
      "source": [
        "Evaluation on new sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ucXWryrPwR9"
      },
      "source": [
        "def evaluate(sentence):\r\n",
        "\r\n",
        "  attention_matrix = np.zeros((target_max_length, input_max_length))\r\n",
        "\r\n",
        "  sentence = dataObject.preprocess_sentence(sentence)\r\n",
        "\r\n",
        "  input_tokens = [input_tokenizer.word_index[i] for i in sentence.split(' ') if i in input_tokenizer.word_index.keys()]\r\n",
        "\r\n",
        "  input_tokens = list(reversed(input_tokens))\r\n",
        "\r\n",
        "  input_tokens = pad_sequences([input_tokens], maxlen=input_max_length, padding='pre', truncating='pre')\r\n",
        "\r\n",
        "  input_tensor = tf.convert_to_tensor(input_tokens)\r\n",
        " \r\n",
        "\r\n",
        "  translation = ''\r\n",
        "\r\n",
        "  encoder_output, encoder_hidden, _ = encoder(input_tensor)\r\n",
        "\r\n",
        "  decoder_input = tf.expand_dims([target_tokenizer.word_index['ssss']], 0)\r\n",
        "\r\n",
        "  decoder_hidden = encoder_hidden\r\n",
        "\r\n",
        "  for t in range(target_max_length):\r\n",
        "\r\n",
        "    predictions, decoder_hidden, attention_weights = decoder(decoder_input, decoder_hidden, encoder_output)\r\n",
        "\r\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\r\n",
        "    attention_matrix[t] = attention_weights.numpy()\r\n",
        "\r\n",
        "    predicted_index = tf.argmax(predictions[0]).numpy()\r\n",
        "\r\n",
        "    translation += target_tokenizer.index_word[predicted_index] + ' '\r\n",
        "\r\n",
        "    if target_tokenizer.index_word[predicted_index] == 'eeee':\r\n",
        "      return translation, sentence, attention_matrix\r\n",
        "\r\n",
        "    # The prediction is fed into the next time step of the decoder\r\n",
        "    decoder_input = tf.expand_dims([predicted_index], 0)\r\n",
        "\r\n",
        "  return translation, sentence, attention_matrix\r\n",
        "\r\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsBmxuUaU6nH"
      },
      "source": [
        "def plot_attention_weights(attention, sentence, predicted_translation):\r\n",
        "  fig = plt.figure(figsize=(10,10))\r\n",
        "  ax = fig.add_subplot(1, 1, 1)\r\n",
        "  ax.matshow(attention, cmap='viridis')\r\n",
        "\r\n",
        "  fontdict = {'fontsize': 14}\r\n",
        "\r\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\r\n",
        "  ax.set_yticklabels([''] + predicted_translation, fontdict=fontdict)\r\n",
        "\r\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "  plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6_kS_lmVFKo"
      },
      "source": [
        "def translate(sentence):\r\n",
        "  translation, sentence, attention_matrix = evaluate(sentence)\r\n",
        "\r\n",
        "  print('Input: %s' % (sentence))\r\n",
        "  print('Predicted translation: {}'.format(translation))\r\n",
        "\r\n",
        "  attention_matrix = attention_matrix[:len(translation.split(' ')), :len(sentence.split(' '))]\r\n",
        "  plot_attention_weights(attention_matrix, sentence.split(' '), translation.split(' '))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw4N3N1KL5YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ea66b0-c0e3-4ec8-d135-a07a35343196"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7f574d67b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKE_L87uLXDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bffd0a5-8ca5-4fd0-8a41-af5dee3120a5"
      },
      "source": [
        "total_val_loss = 0\r\n",
        "for (batch, (input_tensor, target_tensor)) in enumerate(test_data.take(int(steps_per_epoch))):\r\n",
        "    val_batch_loss = test(input_tensor, target_tensor)\r\n",
        "\r\n",
        "    total_val_loss += val_batch_loss\r\n",
        "\r\n",
        "mean_loss = total_val_loss/int(steps_per_epoch)\r\n",
        "\r\n",
        "print(mean_loss.numpy())"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5352502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRF9XDz7W7tM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "outputId": "d310e277-f5fc-4c5f-9c51-c286a58d71f2"
      },
      "source": [
        "translate(u'Wie geht es dir')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 46, 8, 61, 37, 1]\n",
            "[[ 0  2 46  8 61 37  1]]\n",
            "Input: ssss wie geht es dir eeee\n",
            "Predicted translation: how are you are eeee \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "HoFTJYcfLDsE",
        "outputId": "f2dbe56d-10e8-4140-d706-1ab825a47a8d"
      },
      "source": [
        "translate(u'Er will nicht darin verstrickt werden')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 72, 1471, 9, 64, 13, 1]\n",
            "[[   2   72 1471    9   64   13    1]]\n",
            "Input: ssss er will nicht darin verstrickt werden eeee\n",
            "Predicted translation: he can not not in not \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}